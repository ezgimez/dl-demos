{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_demo7_modified.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezgimez/dl-demos/blob/main/dl_demo7_modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR3X8KNXVkGj"
      },
      "source": [
        "\n",
        "# Transformers in Computer Vision\n",
        "\n",
        "\n",
        "\n",
        "Transformer architectures owe their origins in natural language processing (NLP), and indeed form the core of the current state of the art models for most NLP applications.\n",
        "\n",
        "We will now see how to develop transformers for processing image data (and in fact, this line of deep learning research has been gaining a lot of attention in 2021). The *Vision Transformer* (ViT) introduced in [this paper](https://arxiv.org/pdf/2010.11929.pdf) shows how standard transformer architectures can perform very well on image. The high level idea is to extract patches from images, treat them as tokens, and pass them through a sequence of transformer blocks before throwing on a couple of dense classification layers at the very end.\n",
        "\n",
        "\n",
        "Some caveats to keep in mind: \n",
        "- ViT models are very cumbersome to train (since they involve a ton of parameters) so budget accordingly. \n",
        "- ViT models are a bit hard to interpret (even more so than regular convnets).\n",
        "- Finally, while in this notebook we will train a transformer from scratch, ViT models in practice are almost always *pre-trained* on some large dataset (such as ImageNet) before being transferred onto specific training datasets.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1m4W8BCYCG_"
      },
      "source": [
        "# Setup\n",
        "\n",
        "As usual, we start with basic data loading and preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcX3S4xjEoGD",
        "outputId": "9f986c3e-f23d-4596-be23-44e4d964b5dd"
      },
      "source": [
        "!pip install einops # some package"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk0KHIjUDRSH"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL2k3L-mDkDR"
      },
      "source": [
        "torch.manual_seed(42) # to make sure randomness is deterministic\n",
        "\n",
        "DOWNLOAD_PATH = '/data/mnist'\n",
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 1000\n",
        "\n",
        "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
        "                                       transform=transform_mnist)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
        "                                      transform=transform_mnist)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Zi6xFSbU5M"
      },
      "source": [
        "# The ViT Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkMNRs4c4Ja"
      },
      "source": [
        "We will now set up the ViT model. There will be 3 parts to this model:\n",
        "\n",
        "* A *patch embedding* layer that takes an image and tokenizes it. There is some amount of tensor algebra involved here (since we have to slice and dice the input appropriately), and the `einops` package is helpful. We will also add learnable positional encodings as parameters.\n",
        "* A sequence of transformer blocks. This will be a smaller scale replica of the original proposed ViT, except that we will only use 4 blocks in our model (instead of 32 in the actual ViT).\n",
        "* A (dense) classification layer at the end.\n",
        "\n",
        "Further, each transformer block consists of the following components: \n",
        "\n",
        "* A *self-attention* layer with $H$ heads, \n",
        "* A one-hidden-layer (dense) network to collapse the various heads. For the hidden neurons, the original ViT used something called a [GeLU](https://arxiv.org/pdf/1606.08415.pdf) activation function, which is a smooth approximation to the ReLU. For our example, regular ReLUs seem to be working just fine. The original ViT also used Dropout but we won't need it here.\n",
        "* *Layer normalization* preceeding each of the above operations.\n",
        "\n",
        "Some care needs to be taken in making sure the various dimensions of the tensors are matched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAuZ2Nm_DXCB"
      },
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# print(pair(3))\n",
        "# prints (3,3)\n",
        "\n",
        "# print(pair((3,5,7)))\n",
        "# prints (3,5,7)\n",
        "\n",
        "# classes\n",
        "\n",
        "class PreNorm(nn.Module): # normalizes just before applying the function\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim) # check: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.\n",
        "        # normalized_shape argument is `dim`\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module): # this is the one-hidden-layer network mentioned above, that will collapse various \"heads\"\n",
        "     # this is the MLP in Fig.1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(), #nn.GELU(), # we will use classical ReLU and no dropouts\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "   \n",
        "\n",
        "class Attention(nn.Module): # this is the multi-head attention in Fig.1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads # this is because `heads` will be working in parallel\n",
        "        project_out = not (heads == 1 and dim_head == dim) # see comment in line 52\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1) # softmax will be applied to the last dimension\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential( # dimension reduction operations are done with some NN blocks. \n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "        # if project_out is false, you just have nn.Identity()\n",
        "        # if there is only one `head` and `dim`=`inner_dim`, you just return identity\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # prints [100, 17, 64]\n",
        "        # 64 comes from `to_patch_embedding`\n",
        "        b, n, _, h = *x.shape, self.heads # n = number of patches. in our case (28/7=4)^2 = 16.  \n",
        "        # +1 comes from extra learnable class embedding in Fig. 1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1) # chunk(3) because of you want to partition it into q, k, v\n",
        "        # self.to_qkv(x) results in shape [100, 17, 64*8*3], where 64*8=inner_dim\n",
        "        # qkv consists of 3 tensors of shape [100, 17, 64*8]\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "        # this is the line that implements multi-head attention so that we run `h` (heads) amount of self-attention operations in parallel\n",
        "\n",
        "        # print(q.shape)\n",
        "        # prints [100, 8, 17, 64] since we have `heads`=8.\n",
        "           \n",
        "        # matrix multiplication performed as such because you do the matrix multiplication for every pair of (b, h)\n",
        "        # since every batch and every head is acting in 'orthogonal fashion'\n",
        "\n",
        "        # follows Eq. 6 in the appendix of https://arxiv.org/pdf/2010.11929.pdf\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale   \n",
        "        # to obtain (b, h, i, j) in the results tensor, you iterate over all possible d's and compute q(b,h,i,d)*k(b,h,j,d) and sum them up.\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        # follows Eq. 7 in the appendix of https://arxiv.org/pdf/2010.11929.pdf\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        # to obtain (b, h, i, d) in the results tensor, you iterate over all possible j's and compute q(b,h,i,j)*k(b,h,j,d) and sum them up.\n",
        "        # check the dimensions of the above to confirm it corresponds to the matrix summation\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)') # after parallelization with `heads`, you re-arrange the array\n",
        "        # so that it has its initial structure (see line 63) as well as the fact that each head contains information from each patch.\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth): # depth corresponds to L in Fig.1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) # this is the blue block in Fig. 1 RHS\n",
        "            ]))\n",
        "    def forward(self, x): # implements Fig. 1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        # the program crashes if assert statement is not satisfied\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width) # mathematical equation to get num_patches \n",
        "        patch_dim = channels * patch_height * patch_width # we have channels=1 in our case.\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            # obtains h*w amount of patches of size p1*p2*c for every batch, b.\n",
        "            # in the rest of the functions, h*w=n, i.e. number of patches\n",
        "            nn.Linear(patch_dim, dim), # nn.Linear output dimensions is `dim` so that it matches the input dimension of Transformer\n",
        "            # see line 128\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity() # just a placeholder for identity operator\n",
        "\n",
        "        self.mlp_head = nn.Sequential( # this is the MLP-head in Fig. 1 https://arxiv.org/pdf/2010.11929.pdf\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        # print(x.shape)\n",
        "        # prints [100, 16, 64] since `dim`=64\n",
        "        b, n, _ = x.shape # n = h*w, i.e. number of patches, see line 108\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b) # repeat comes from einops: https://einops.rocks/api/repeat/\n",
        "        # print(cls_tokens.shape)\n",
        "        # prints [100, 1, 64], where b=100\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        # x is now of shape [100, 17, 64], since we know added extra learnable [class] embedding\n",
        "        x += self.pos_embedding[:, :(n + 1)] \n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0] # since we have a learnable class embedding \n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vD4p1EyWHbD"
      },
      "source": [
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1, dim=64, depth=4, heads=8, mlp_dim=128)\n",
        "# image size is 28x28. patch size is 7x7. 28 is divisible by 7 -- expected behaviour.\n",
        "# we will use 4 blocks in our model, i.e. `depth=4`.\n",
        "# `channels=1` since we have black-white image.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7gfKWm0ggw"
      },
      "source": [
        "Let's see how the model looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riukWYK5WIkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e4f39e-aff8-4a00-b07c-0f8b125224c2"
      },
      "source": [
        "model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (to_patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=7, p2=7)\n",
              "    (1): Linear(in_features=49, out_features=64, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=64, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5XbEtJbtiG"
      },
      "source": [
        "This is it -- 4 transformer blocks, followed by a linear classification layer. Let us quickly see how many trainable parameters are present in this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTN9-kFMbdXn",
        "outputId": "679d9f36-7ce4-4284-a1d9-4b95eb8fad0d"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(model))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "597002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgSVq3NqcKT3"
      },
      "source": [
        "About half a million. Not too bad; the bigger NLP type models have several tens of millions of parameters. But since we are training on MNIST this should be more than sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mnUt-GXYYv3"
      },
      "source": [
        "# Training and testing\n",
        "\n",
        "All done! We can now train the ViT model. The following again is boilerplate code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxnADQNDpgA"
      },
      "source": [
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1) # https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html \n",
        "        loss = F.nll_loss(output, target) # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html. useful for classication problem with C classes\n",
        "        # as seen in the link above, log_softmax and nll_loss are coupled together...\n",
        "        # as an alternative to CrossEntropyLoss(), the operations are identical. \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moywc015DrAg"
      },
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "    \n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum() # .eq() is to compare \n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbCf2V--cohk"
      },
      "source": [
        "\n",
        "The following will take a bit of time (on CPU).  Each epoch should take about 2 to 3 minutes. At the end of training, we should see upwards of 95% test accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0Hb-TKDwbA",
        "outputId": "048360aa-61f5-4aba-d6c6-f7a4f274b8ec"
      },
      "source": [
        "N_EPOCHS = 3\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "[    0/60000 (  0%)]  Loss: 2.5616\n",
            "[10000/60000 ( 17%)]  Loss: 0.1940\n",
            "[20000/60000 ( 33%)]  Loss: 0.2094\n",
            "[30000/60000 ( 50%)]  Loss: 0.2383\n",
            "[40000/60000 ( 67%)]  Loss: 0.1699\n",
            "[50000/60000 ( 83%)]  Loss: 0.2236\n",
            "\n",
            "Average test loss: 0.1999  Accuracy: 9406/10000 (94.06%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/60000 (  0%)]  Loss: 0.1620\n",
            "[10000/60000 ( 17%)]  Loss: 0.1637\n",
            "[20000/60000 ( 33%)]  Loss: 0.1777\n",
            "[30000/60000 ( 50%)]  Loss: 0.1854\n",
            "[40000/60000 ( 67%)]  Loss: 0.1266\n",
            "[50000/60000 ( 83%)]  Loss: 0.1507\n",
            "\n",
            "Average test loss: 0.0996  Accuracy: 9690/10000 (96.90%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/60000 (  0%)]  Loss: 0.2036\n",
            "[10000/60000 ( 17%)]  Loss: 0.0808\n",
            "[20000/60000 ( 33%)]  Loss: 0.0769\n",
            "[30000/60000 ( 50%)]  Loss: 0.0326\n",
            "[40000/60000 ( 67%)]  Loss: 0.1627\n",
            "[50000/60000 ( 83%)]  Loss: 0.0987\n",
            "\n",
            "Average test loss: 0.1044  Accuracy: 9664/10000 (96.64%)\n",
            "\n",
            "Execution time: 490.28 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35jvT53FuKNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c7b698-8d56-4566-8b12-eff6d58520c7"
      },
      "source": [
        "evaluate(model, test_loader, test_loss_history)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average test loss: 0.1044  Accuracy: 9664/10000 (96.64%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}